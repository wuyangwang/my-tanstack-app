# 实时流式听写 (Live Dictation) 实现方案

实现实时流式听写比处理静态文件更复杂，主要因为 Whisper 模型设计用于处理 30 秒长的音频块。要实现“实时”感，需要采用 ASR (Automatic Speech Recognition，自动语音识别) 的流式处理策略：

### 1. 音频切片与滑动窗口 (Sliding Window)
*   **采样**：持续获取麦克风音频流。
*   **切片**：每隔 0.5 到 1 秒切出一个片段。
*   **窗口叠加**：维护一个 3-5 秒的滑动窗口。每次将“旧音频 + 新片段”组合输入模型，利用上下文提高准确率。

### 2. VAD (语音活动检测)
*   在音频进入模型前，使用轻量级 VAD 判断是否有人声。
*   静音或噪音环境下不触发推理，节省 GPU/CPU 资源。

### 3. 增量解码 (Incremental Decoding)
*   **中间结果**：实时显示模型当前最确信的文字。
*   **结果修正**：随着语境完整，模型会自动修正之前识别错误的词。
*   **提交 (Commitment)**：检测到长时间停顿（如 1 秒）后，认为话语结束，存入历史记录并清空缓冲区。

### 4. 性能优化 (WebGPU)
*   **延迟敏感**：处理 1 秒音频的时间必须远小于 1 秒（建议在 100ms 内）。
*   **WebGPU**：提供足够算力，确保后台快速循环推理不阻塞 UI。

### 5. 挑战：幻觉与漂移
*   在安静或只有噪音时，Whisper 可能输出幻觉（如重复特定词汇）。
*   **解决**：通过音频能量检测或模型置信度过滤低质量结果。

### 总结流程
**麦克风数据流 -> VAD 过滤 -> 滑动窗口缓冲 -> WebGPU 快速推理 -> UI 实时显示中间结果 -> 检测到停顿后合并结果。**
